{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1404446",
   "metadata": {},
   "source": [
    "## Get started with TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a657294",
   "metadata": {},
   "source": [
    "In machine learning, to improve something you often need to be able to measure it. TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b5f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc68a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e3c6743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7c7072",
   "metadata": {},
   "source": [
    "Using the MNIST dataset as the example, normalize the data and write a function that creates a simple Keras model for classifying the images into 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321874d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "def create_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28), name='layers_flatten'),\n",
    "    tf.keras.layers.Dense(512, activation='relu', name='layers_dense'),\n",
    "    tf.keras.layers.Dropout(0.2, name='layers_dropout'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax', name='layers_dense_2')\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7391674f",
   "metadata": {},
   "source": [
    "Using TensorBoard with Keras Model.fit()\n",
    "\n",
    "When training with Keras's Model.fit(), adding the tf.keras.callbacks.TensorBoard callback ensures that logs are created and stored. Additionally, enable histogram computation every epoch with histogram_freq=1 (this is off by default)\n",
    "\n",
    "Place the logs in a timestamped subdirectory to allow easy selection of different training runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819e5e1e",
   "metadata": {},
   "source": [
    "### Using TensorBoard with Keras Model.fit()\n",
    "When training with Keras's Model.fit(), adding the tf.keras.callbacks.TensorBoard callback ensures that logs are created and stored. Additionally, enable histogram computation every epoch with histogram_freq=1 (this is off by default)\n",
    "\n",
    "Place the logs in a timestamped subdirectory to allow easy selection of different training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa58c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\miniconda3\\envs\\tf_ml\\lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.8941 - loss: 0.3590 - val_accuracy: 0.9691 - val_loss: 0.1025\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9694 - loss: 0.1005 - val_accuracy: 0.9744 - val_loss: 0.0819\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 11ms/step - accuracy: 0.9789 - loss: 0.0682 - val_accuracy: 0.9771 - val_loss: 0.0703\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.9850 - loss: 0.0507 - val_accuracy: 0.9750 - val_loss: 0.0794\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 9ms/step - accuracy: 0.9868 - loss: 0.0407 - val_accuracy: 0.9808 - val_loss: 0.0666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x248eac2f0d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=5, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3989faf2",
   "metadata": {},
   "source": [
    "Start TensorBoard through the command line or within a notebook experience. The two interfaces are generally the same. In notebooks, use the %tensorboard line magic. On the command line, run the same command without \"%\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a4c3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-29bb99c4a320c0e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-29bb99c4a320c0e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23684a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 16904), started 0:00:07 ago. (Use '!kill 16904' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-dda2d77863282f5f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-dda2d77863282f5f\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c695e3",
   "metadata": {},
   "source": [
    "A brief overview of the visualizations created in this example and the dashboards (tabs in top navigation bar) where they can be found:\n",
    "\n",
    "- **Scalars** show how the loss and metrics change with every epoch. You can use them to also track training    speed, learning rate, and other scalar values. Scalars can be found in the Time Series or Scalars dashboards.\n",
    "- **Graphs** help you visualize your model. In this case, the Keras graph of layers is shown which can help you ensure it is built correctly. Graphs can be found in the Graphs dashboard.\n",
    "- **Histograms** and Distributions show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way. Histograms can be found in the Time - Series or Histograms dashboards. Distributions can be found in the Distributions dashboard.\n",
    "  \n",
    "Additional TensorBoard dashboards are automatically enabled when you log other types of data. For example, the Keras TensorBoard callback lets you log images and embeddings as well. You can see what other dashboards are available in TensorBoard by clicking on the \"inactive\" dropdown towards the top right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38418534",
   "metadata": {},
   "source": [
    "### Using TensorBoard with other methods\n",
    "\n",
    "When training with methods such as tf.GradientTape(), use tf.summary to log the required information.\n",
    "\n",
    "Use the same dataset as above, but convert it to tf.data.Dataset to take advantage of batching capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd6f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(60000).batch(64)\n",
    "test_dataset = test_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac9fbe",
   "metadata": {},
   "source": [
    "The training code follows the advanced quickstart tutorial, but shows how to log metrics to TensorBoard. Choose loss and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55466ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08ddac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our metrics\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe455674",
   "metadata": {},
   "source": [
    "Define the training and test functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94b08e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, x_train, y_train):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(x_train, training=True)\n",
    "    loss = loss_object(y_train, predictions)\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(y_train, predictions)\n",
    "\n",
    "def test_step(model, x_test, y_test):\n",
    "  predictions = model(x_test)\n",
    "  loss = loss_object(y_test, predictions)\n",
    "\n",
    "  test_loss(loss)\n",
    "  test_accuracy(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aafc646",
   "metadata": {},
   "source": [
    "Set up summary writers to write the summaries to disk in a different logs directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b055506",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55f824",
   "metadata": {},
   "source": [
    "Start training. Use tf.summary.scalar() to log metrics (loss and accuracy) during training/testing within the scope of the summary writers to write the summaries to disk. You have control over which metrics to log and how often to do it. Other tf.summary functions enable logging other types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52416504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.24463534355163574, Accuracy: 92.86328887939453, Test Loss: 0.12463685125112534, Test Accuracy: 96.48500061035156\n",
      "Epoch 2, Loss: 0.10592766106128693, Accuracy: 96.87000274658203, Test Loss: 0.08397915214300156, Test Accuracy: 97.37999725341797\n",
      "Epoch 3, Loss: 0.07417217642068863, Accuracy: 97.77333068847656, Test Loss: 0.07714098691940308, Test Accuracy: 97.64999389648438\n",
      "Epoch 4, Loss: 0.0554547943174839, Accuracy: 98.26499938964844, Test Loss: 0.06713751703500748, Test Accuracy: 97.87999725341797\n",
      "Epoch 5, Loss: 0.042795874178409576, Accuracy: 98.64500427246094, Test Loss: 0.06105022132396698, Test Accuracy: 98.0999984741211\n"
     ]
    }
   ],
   "source": [
    "model = create_model()  # Reset model\n",
    "optimizer = tf.keras.optimizers.Adam()  # Reinitialize optimizer # reset our model\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  for (x_train, y_train) in train_dataset:\n",
    "    train_step(model, optimizer, x_train, y_train)\n",
    "  with train_summary_writer.as_default():\n",
    "    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "  for (x_test, y_test) in test_dataset:\n",
    "    test_step(model, x_test, y_test)\n",
    "  with test_summary_writer.as_default():\n",
    "    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "  \n",
    "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "  print (template.format(epoch+1,\n",
    "                         train_loss.result(), \n",
    "                         train_accuracy.result()*100,\n",
    "                         test_loss.result(), \n",
    "                         test_accuracy.result()*100))\n",
    "\n",
    "  # Reset metrics every epoch\n",
    "  train_loss = tf.keras.metrics.Mean()  # Creating a new Mean instance\n",
    "  test_loss = tf.keras.metrics.Mean()   # This resets it automatically\n",
    "  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "  test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7872c3aa",
   "metadata": {},
   "source": [
    "Open TensorBoard again, this time pointing it at the new log directory. We could have also started TensorBoard to monitor training while it progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a531e8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-465ac00ddb9a9894\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-465ac00ddb9a9894\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/gradient_tape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f1fef",
   "metadata": {},
   "source": [
    "That's it! You have now seen how to use TensorBoard both through the Keras callback and through tf.summary for more custom scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
